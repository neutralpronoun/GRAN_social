INFO  | 06/29/2022 06:18:16 AM | File run_exp.py           | Line 26    | Writing log file to /home/alex/Projects/GRAN_social/exp/GRAN/FACEBOOK_LARGE/log_exp_17654.txt
INFO  | 06/29/2022 06:18:16 AM | File run_exp.py           | Line 27    | Exp instance id = 17654
INFO  | 06/29/2022 06:18:16 AM | File run_exp.py           | Line 28    | Exp comment = None
INFO  | 06/29/2022 06:18:16 AM | File run_exp.py           | Line 29    | Config =
INFO  | 06/29/2022 06:18:21 AM | File gran_runner.py       | Line 135   | Train/val/test = 160/40/40
INFO  | 06/29/2022 06:18:22 AM | File gran_runner.py       | Line 148   | No Edges vs. Edges in training set = 89.22312601985614
INFO  | 06/29/2022 06:18:54 AM | File gran_runner.py       | Line 290   | NLL Loss @ epoch 20001 iteration 00000001 = 0.1534217894077301
ERROR | 06/29/2022 06:18:56 AM | File run_exp.py           | Line 46    | Traceback (most recent call last):
  File "/home/alex/Projects/GRAN_social/run_exp.py", line 44, in main
    runner.train()
  File "/home/alex/Projects/GRAN_social/runner/gran_runner.py", line 275, in train
    train_loss.backward()
  File "/home/alex/anaconda3/envs/pytorch_graphs/lib/python3.10/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/alex/anaconda3/envs/pytorch_graphs/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 162.00 MiB (GPU 0; 7.80 GiB total capacity; 6.45 GiB already allocated; 119.88 MiB free; 6.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

